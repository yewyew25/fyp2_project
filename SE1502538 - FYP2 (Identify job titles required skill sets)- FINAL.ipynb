{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">---------------</font>\n",
    "<font size=\"30\">Data processing</font>\n",
    "<font size=\"30\">---------------</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">1.0 Prepare data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "try: \n",
    "    client = MongoClient()\n",
    "    print(\"Connected successfully!!!\") \n",
    "except:\n",
    "    print(\"Could not connect to MongoDB\") \n",
    "\n",
    "db = client.job_advert_db #select database\n",
    "collection = db.job_advert #select collection to insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "job_advert_collection = collection.find({})\n",
    "print(\"Amount of data collected:\" , job_advert_collection.count())\n",
    "\n",
    "job_adverts_title = []\n",
    "job_adverts_desc = []\n",
    "\n",
    "for ja in job_advert_collection:\n",
    "    job_adverts_title.append(ja['title'])\n",
    "    job_adverts_desc.append(ja['desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_adverts_title))\n",
    "print(len(job_adverts_desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">2.0 Data Preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions #expand the contractions \"you're -> you are\"\n",
    "import inflect #convert plural to singular nouns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords #filter english words \"is\", \"this\", \"there\", etc.\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import tqdm\n",
    "from bson.json_util import dumps\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "word_arr = ['experience','year', 'location', 'apply','relevant', 'include',\n",
    "                'job', 'advert', 'able', 'based', 'key', 'resume', \n",
    "                'company', 'meet', 'change', 'eg', 'salary', 'please', \n",
    "            'payroll', 'incident', 'related', 'interested', 'us', \n",
    "            'singaporecompany', 'ltd', 'via', 'pte', 'create', 'across', \n",
    "            'standard', 'employer', 'holiday', 'new', 'participate', 'people', 'also', 'allow', 'always'\n",
    "           ]\n",
    "\n",
    "word_arr = stem_words(word_arr)\n",
    "word_arr = lemmatize_verbs(word_arr)\n",
    "\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(word_arr)\n",
    "\n",
    "print(stopset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Denoise Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    processed_text = []\n",
    "    pbar = tqdm.tqdm(total=len(text))\n",
    "    for t in text:\n",
    "        text = strip_html(t)\n",
    "        text = remove_between_square_brackets(t)\n",
    "        processed_text.append(text)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4827/4827 [00:00<00:00, 5085.37it/s]\n"
     ]
    }
   ],
   "source": [
    "job_adverts_desc = denoise_text(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Replace contractions in string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    text_arr = []\n",
    "    pbar = tqdm.tqdm(total=len(text))\n",
    "    for t in text:\n",
    "        t = contractions.fix(t)\n",
    "        text_arr.append(t)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return text_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4827/4827 [00:54<00:00, 88.05it/s]\n"
     ]
    }
   ],
   "source": [
    "job_adverts_desc = replace_contractions(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4827/4827 [00:19<00:00, 249.67it/s]\n"
     ]
    }
   ],
   "source": [
    "temp_arr = []\n",
    "pbar = tqdm.tqdm(total=len(job_adverts_desc))\n",
    "for ja in job_adverts_desc:\n",
    "    temp_arr.append(nltk.word_tokenize(ja))\n",
    "    pbar.update(1)\n",
    "job_adverts_desc = temp_arr\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Normalization ((1) stemming, (2) lemmatization, and (3) everything else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopset:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize_without_stem(words):\n",
    "    pbar = tqdm.tqdm(total=len(words))\n",
    "    word_arr = []\n",
    "    for word in words:\n",
    "        word = remove_non_ascii(word)\n",
    "        word = to_lowercase(word)\n",
    "        word = remove_punctuation(word)\n",
    "        word = replace_numbers(word)\n",
    "        word = remove_stopwords(word)\n",
    "        word_arr.append(word)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return word_arr\n",
    "\n",
    "def normalize_stem(words):\n",
    "    pbar = tqdm.tqdm(total=len(words))\n",
    "    word_arr = []\n",
    "    for word in words:\n",
    "        word = remove_non_ascii(word)\n",
    "        word = to_lowercase(word)\n",
    "        word = remove_punctuation(word)\n",
    "        word = replace_numbers(word)\n",
    "        word = remove_stopwords(word)\n",
    "        word = stem_words(word)\n",
    "        word = lemmatize_verbs(word)\n",
    "        word_arr.append(word)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return word_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4827/4827 [00:08<00:00, 590.49it/s]\n"
     ]
    }
   ],
   "source": [
    "job_adverts_desc_nostem = normalize_without_stem(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4827/4827 [00:56<00:00, 85.90it/s]\n"
     ]
    }
   ],
   "source": [
    "job_adverts_desc = normalize_stem(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5 Remove useless words stick with numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_number_word(jobadverts):\n",
    "    import re\n",
    "    pbar = tqdm.tqdm(total=10)\n",
    "    for i in range(1,11):\n",
    "        for desc in jobadverts:\n",
    "            for i in desc:\n",
    "                if(re.search(r'\\w*\\d\\w*', i)):\n",
    "                    desc.remove(str(i))\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    return jobadverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:22<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "job_adverts_desc = remove_number_word(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:30<00:00,  3.29s/it]\n"
     ]
    }
   ],
   "source": [
    "job_adverts_desc_nostem = remove_number_word(job_adverts_desc_nostem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_arr(jobadverts):\n",
    "    temp_arr = []\n",
    "\n",
    "    for words in jobadverts:\n",
    "        for word in words:\n",
    "            temp_arr.append(word)\n",
    "    return temp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_adverts_desc_arr = to_arr(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_adverts_desc_nostem_arr = to_arr(job_adverts_desc_nostem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lead', 'loc', 'enterpriseunderstand', 'cur', 'process', 'valu', 'outcomes', 'structure', 'larg', 'amount']\n",
      "['leading', 'local', 'enterpriseunderstand', 'current', 'processes', 'valuable', 'outcomesable', 'structure', 'large', 'amount']\n"
     ]
    }
   ],
   "source": [
    "print(job_adverts_desc_arr[:10])\n",
    "print(job_adverts_desc_nostem_arr[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to backup data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('job_advert_desc_arr_dump', 'wb') as bf:\n",
    "    pickle.dump(job_adverts_desc_arr, bf)\n",
    "with open('job_advert_desc_nostem_arr_dump', 'wb') as bf:\n",
    "    pickle.dump(job_adverts_desc_nostem_arr, bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('job_advert_desc_dump', 'wb') as bf:\n",
    "    pickle.dump(job_adverts_desc, bf)\n",
    "with open('job_advert_title_dump', 'wb') as bf:\n",
    "    pickle.dump(job_adverts_title , bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to load data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "job_adverts_desc = []\n",
    "with open('job_advert_desc_arr_dump', 'rb') as lf:\n",
    "    job_adverts_desc_arr = pickle.load(lf)\n",
    "with open('job_advert_desc_nostem_arr_dump', 'rb') as lf:\n",
    "    job_adverts_desc_nostem_arr = pickle.load(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "job_adverts_desc = []\n",
    "with open('job_advert_desc_dump', 'rb') as lf:\n",
    "    job_adverts_desc = pickle.load(lf)\n",
    "with open('job_advert_title_dump', 'rb') as lf:\n",
    "    job_adverts_title = pickle.load(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 897568 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': job_adverts_desc_nostem_arr}, index = job_adverts_desc_arr)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lead', 'loc', 'enterpriseunderstand', 'cur', 'process', 'valu', 'outcomes', 'structure', 'larg', 'amount']\n",
      "['leading', 'local', 'enterpriseunderstand', 'current', 'processes', 'valuable', 'outcomesable', 'structure', 'large', 'amount']\n",
      "[['lead', 'loc', 'enterpriseunderstand', 'cur', 'process', 'valu', 'outcomes', 'structure', 'larg', 'amount', 'inform', 'within', 'process', 'frameworkjob', 'scopemappingreview', 'ex', 'busy', 'process', 'reimagin', 'improv', 'on', 'maxim', 'custom', 'valu', 'minim', 'wast', 'incorp', 'automationfacilitatelead', 'process', 'workshop', 'involv', 'elicit', 'process', 'requir', 'lia', 'users', 'interpret', 'busy', 'requir', 'digit', 'solutioncr', 'project', 'plan', 'contain', 'project', 'goal', 'mileston', 'resourcesmonit', 'meas', 'provid', 'feedback', 'process', 'performancelia', 'man', 'coordin', 'process', 'improv', 'project', 'activitiesrequir', 'enjoy', 'vary', 'work', 'widerang', 'rol', 'immers', 'vary', 'diff', 'project', 'see', 'big', 'picturework', 'busy', 'team', 'digit', 'depart', 'fig', 'best', 'way', 'design', 'new', 'process', 'solv', 'processrel', 'problemsstrong', 'analys', 'background', 'problem', 'solv', 'good', 'diagram', 'commun', 'skillsknowledg', 'process', 'map', 'busy', 'process', 'reengineeringperform', 'analys', 'busy', 'process', 'workflow', 'object', 'find', 'improv', 'automatedknowledg', 'busy', 'process', 'model', 'not', 'bpmn', 'epc', 'etc', 'busy', 'process', 'tool', 'ar', 'interest', 'candid', 'wish', 'advert', 'posit', 'pleas', 'email', 'us', 'upd', 'cop', 'ms', 'word', 'form', 'regret', 'shortl', 'candid', 'notifiede', 'lic', 'reg', 'jimmy', 'low'], ['lead', 'loc', 'enterprisestrong', 'knowledg', 'enterpr', 'hardw', 'softwaremicrosoft', 'cert', 'system', 'engin', 'mcse', 'itil', 'advantageresponsibilitiesprovid', 'support', 'end', 'us', 'includ', 'enterpr', 'hardw', 'softwareprovid', 'admin', 'serv', 'desk', 'support', 'intern', 'customerprovid', 'solv', 'end', 'us', 'request', 'problem', 'via', 'multipl', 'channel', 'phon', 'email', 'remot', 'serv', 'serv', 'man', 'system', 'etc', 'support', 'teleconf', 'audiov', 'infrastruct', 'servicesparticip', 'project', 'requiredrequirementexcel', 'techn', 'knowledg', 'skil', 'window', 'osdeep', 'understand', 'hardw', 'softw', 'microsoft', 'off', 'suit', 'visio', 'etc', 'admin', 'act', 'direct', 'domain', 'group', 'policy', 'microsoft', 'off', 'enterpr', 'softw', 'instal', 'network', 'system', 'troubleshoot', 'sec', 'patch', 'etcexpery', 'support', 'follow', 'apply', 'plusmobl', 'dev', 'managementmicrosoft', 'system', 'cent', 'config', 'managermicrosoft', 'lyncskypemicrosoft', 'bitlockermicrosoft', 'cert', 'system', 'engin', 'mcse', 'itil', 'plusinterest', 'candid', 'wish', 'advert', 'posit', 'pleas', 'email', 'us', 'upd', 'cop', 'ms', 'word', 'form', 'regret', 'shortl', 'candid', 'not', 'ea', 'lic', 'reg', 'jimmy', 'low']]\n",
      "['Business Process Engineer | Local MNC | Central', 'IT Desktop Support | Local MNC | Central']\n"
     ]
    }
   ],
   "source": [
    "print(job_adverts_desc_arr[:10])\n",
    "print(job_adverts_desc_nostem_arr[:10])\n",
    "print(job_adverts_desc[:2])\n",
    "print(job_adverts_title[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"20\">3.0 Finding useful keywords  TF-IDF</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uSER\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'n', 'p', 'r', 'u', 'v', 'w', 'x'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix #need this if you want to save tfidf_matrix\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "#max value max_df is 0.80 output same shape, so deduct to 0.50\n",
    "tf = TfidfVectorizer(max_df=0.8, min_df=0.010, tokenizer=identity_tokenizer, \n",
    "                     analyzer='word', stop_words = stopset, sublinear_tf=True, lowercase=False, \n",
    "                     use_idf=True)\n",
    "tfidf_matrix =  tf.fit_transform(job_adverts_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4827, 1425)\n",
      "['abap', 'abreast', 'academ', 'acc', 'acceiv', 'accel', 'access', 'accommod', 'accompl', 'accord', 'account', 'achiev', 'acquir', 'acquisit', 'act', 'acum', 'ad', 'adapt', 'addit', 'address', 'adequ', 'adh', 'adhoc', 'adjust', 'admin', 'adob', 'adopt', 'adv', 'advoc', 'affect', 'ag', 'agil', 'agilescr', 'agr', 'agree', 'ai', 'aim', 'ajax', 'alert', 'algorithm', 'align', 'alloc', 'along', 'alongsid', 'altern', 'amaz', 'amazon', 'amb', 'ambigu', 'ambit', 'among', 'analys', 'analyst', 'android', 'angul', 'angulars', 'anoth', 'answ', 'anticip', 'antivir', 'anywh', 'ap', 'apac', 'apach', 'appl', 'apprecy', 'approach', 'appropry', 'approv', 'aptitud', 'ar', 'architect', 'area', 'around', 'arrang', 'art', 'artic', 'ask', 'aspect', 'aspnet', 'ass', 'assess', 'asset', 'assign', 'assist', 'assocy', 'assum', 'attend', 'attitud', 'attract', 'attribut', 'audit', 'audy', 'auth', 'autom', 'autonom', 'avail', 'aw', 'award', 'az', 'b', 'bachel', 'back', 'backend', 'background', 'backlog', 'backup', 'bahas', 'bal', 'bank', 'bash', 'batch', 'becom', 'behalf', 'behavy', 'believ', 'benefit', 'best', 'bet', 'beyond', 'bi', 'big', 'bil', 'bind', 'blu', 'board', 'bon', 'book', 'boot', 'bootstrap', 'bottleneck', 'box', 'brand', 'bridg', 'brief', 'bring', 'broad', 'brows', 'bs', 'budget', 'bug', 'build', 'busy', 'button', 'buy', 'c', 'cal', 'campaign', 'candid', 'cando', 'cap', 'capac', 'capit', 'capitasingaporecom', 'capt', 'car', 'card', 'carry', 'cas', 'cash', 'categ', 'cc', 'cent', 'cert', 'chain', 'challeng', 'champ', 'channel', 'chant', 'charact', 'charg', 'chart', 'check', 'chin', 'chip', 'choos', 'cicd', 'cis', 'cisco', 'cissp', 'cit', 'citrix', 'city', 'clar', 'class', 'cle', 'clear', 'clevel', 'cli', 'click', 'cliento', 'clos', 'cloud', 'clust', 'coach', 'cod', 'col', 'collab', 'colleagu', 'collect', 'com', 'combin', 'comfort', 'command', 'commens', 'commerc', 'commit', 'common', 'commun', 'comp', 'compat', 'compens', 'compet', 'competit', 'compil', 'complet', 'complex', 'comply', 'compon', 'compr', 'comprehend', 'comput', 'computertelecommun', 'conceiv', 'concern', 'concur', 'condit', 'conduc', 'conf', 'confid', 'config', 'confirm', 'conflict', 'conform', 'conjunct', 'connect', 'consid', 'consist', 'consolid', 'const', 'constraint', 'construct', 'consult', 'consum', 'cont', 'contact', 'contain', 'context', 'contin', 'continu', 'contract', 'contribut', 'control', 'convers', 'convert', 'coop', 'coordin', 'cop', 'cor', 'corp', 'correct', 'cost', 'could', 'country', 'cours', 'cov', 'craft', 'cred', 'credit', 'crit', 'criter', 'crm', 'cross', 'crossfunct', 'cryst', 'css', 'cult', 'cur', 'cury', 'custom', 'cut', 'cv', 'cyb', 'cybersec', 'cyc', 'dai', 'daryl', 'dashboard', 'dat', 'databas', 'datac', 'datadr', 'day', 'daytoday', 'db', 'deadlin', 'deal', 'debug', 'decid', 'ded', 'deep', 'defect', 'defin', 'definit', 'degr', 'degreediplom', 'del', 'delight', 'delivery', 'demand', 'demonst', 'dent', 'dep', 'depart', 'depend', 'deploy', 'der', 'describ', 'design', 'desir', 'desk', 'desktop', 'detail', 'detect', 'determin', 'dev', 'develop', 'devop', 'dhcp', 'diagnos', 'diagnost', 'diagram', 'diff', 'differenty', 'digit', 'diplom', 'diplomadegr', 'direct', 'dis', 'disast', 'disciplin', 'discov', 'discrimin', 'discuss', 'display', 'disrupt', 'distribut', 'divers', 'divid', 'dns', 'dock', 'docu', 'domain', 'dot', 'draw', 'driv', 'due', 'dur', 'duty', 'dynam', 'ea', 'eag', 'eap', 'ear', 'east', 'easy', 'eclips', 'ecommerc', 'econom', 'ecosystem', 'edg', 'edit', 'educ', 'effect', 'efficy', 'effort', 'eight', 'eith', 'el', 'elect', 'electron', 'elev', 'elig', 'email', 'embed', 'embrac', 'emerg', 'emphas', 'empow', 'en', 'enco', 'encrypt', 'end', 'endpoint', 'endtoend', 'endus', 'energet', 'energy', 'enforc', 'eng', 'engin', 'engl', 'enh', 'enjoy', 'ens', 'ent', 'enterpr', 'enthusiast', 'entir', 'entrepr', 'entry', 'environ', 'eq', 'equ', 'equip', 'er', 'erp', 'esc', 'espec', 'ess', 'est', 'estim', 'etc', 'eth', 'etl', 'europ', 'ev', 'evalu', 'every', 'everyon', 'everyth', 'evolv', 'ex', 'exampl', 'exceiv', 'excel', 'excess', 'exchang', 'excit', 'execut', 'exerc', 'exhibit', 'exp', 'expand', 'expect', 'expert', 'expl', 'explain', 'expos', 'express', 'extend', 'extern', 'extract', 'ey', 'fac', 'facebook', 'facil', 'facilit', 'fact', 'fail', 'fair', 'famili', 'famy', 'fast', 'fastpac', 'fault', 'feas', 'feat', 'fee', 'feedback', 'feel', 'field', 'fifteen', 'fifty', 'fil', 'fin', 'find', 'fintech', 'firewal', 'firm', 'first', 'fit', 'fiv', 'five hundred', 'five thousand', 'fix', 'flex', 'flow', 'flu', 'foc', 'focus', 'follow', 'followup', 'forecast', 'form', 'fortun', 'forty', 'forward', 'fost', 'four', 'four thousand', 'four thousand, five hundred', 'fram', 'framework', 'fre', 'frequ', 'fresh', 'friday', 'friend', 'front', 'frontend', 'ful', 'fulfil', 'fulltim', 'fulltimeexpery', 'fulltimes', 'fun', 'funct', 'fund', 'funda', 'fut', 'gain', 'gap', 'gateway', 'gath', 'gen', 'gend', 'geograph', 'get', 'git', 'giv', 'glob', 'go', 'goal', 'good', 'googl', 'govern', 'gradu', 'graph', 'grasp', 'gre', 'greet', 'group', 'grow', 'guid', 'guidelin', 'hadoop', 'hand', 'handl', 'handson', 'hap', 'hard', 'hardw', 'hardwaresoftw', 'head', 'headquart', 'heal', 'healthc', 'healthy', 'hear', 'heart', 'help', 'helpdesk', 'hesh', 'hibern', 'high', 'highest', 'highlevel', 'highlight', 'highq', 'hir', 'hish', 'hist', 'hiv', 'hoc', 'hold', 'hom', 'hong', 'hospit', 'host', 'hour', 'hp', 'hr', 'html', 'http', 'https', 'hub', 'hum', 'hybrid', 'hyperv', 'ibm', 'ict', 'id', 'idea', 'ident', 'ie', 'ii', 'illust', 'im', 'immedy', 'impact', 'impl', 'import', 'improv', 'incom', 'incorp', 'increas', 'incumb', 'ind', 'indep', 'independ', 'individ', 'indones', 'indust', 'industry', 'influ', 'inform', 'infotech', 'infotechposit', 'infrastruct', 'inh', 'init', 'innov', 'input', 'ins', 'insid', 'insight', 'inspir', 'instal', 'institut', 'instruct', 'int', 'integr', 'intellig', 'interact', 'interfac', 'intermedy', 'intern', 'internet', 'interperson', 'interpret', 'interview', 'introduc', 'inv', 'invest', 'investig', 'invit', 'invo', 'involv', 'io', 'iot', 'ip', 'iso', 'issu', 'itcomput', 'item', 'itil', 'japan', 'jav', 'javascrib', 'jboss', 'jd', 'jenkin', 'jir', 'jms', 'join', 'journey', 'jquery', 'js', 'json', 'jsp', 'judg', 'junit', 'juny', 'kafk', 'keen', 'keep', 'kind', 'kl', 'know', 'knowledg', 'kong', 'kpi', 'kual', 'kubernet', 'lab', 'lan', 'landscap', 'langu', 'laptop', 'larg', 'largesc', 'largest', 'last', 'lat', 'latest', 'launch', 'law', 'lay', 'layout', 'lead', 'lean', 'learn', 'least', 'leav', 'leg', 'legisl', 'less', 'let', 'lev', 'level', 'lia', 'liaison', 'libr', 'lic', 'licens', 'lif', 'lifecyc', 'light', 'lik', 'limit', 'lin', 'link', 'linux', 'list', 'liv', 'load', 'log', 'long', 'longterm', 'look', 'lot', 'lov', 'low', 'lump', 'mac', 'machin', 'mad', 'mail', 'main', 'maint', 'maintain', 'maj', 'mak', 'malays', 'man', 'mand', 'mandarin', 'manpow', 'manufact', 'many', 'map', 'marit', 'market', 'mast', 'mat', 'match', 'mathem', 'matrix', 'mav', 'maxim', 'may', 'mean', 'meas', 'mech', 'med', 'memb', 'ment', 'mess', 'method', 'methodolog', 'metic', 'microserv', 'microsoft', 'middlew', 'might', 'migr', 'mil', 'mileston', 'min', 'mind', 'mindset', 'minim', 'miss', 'mitig', 'mnc', 'mobl', 'mod', 'model', 'modern', 'mom', 'mon', 'monday', 'mongodb', 'monit', 'month', 'monthexpery', 'monthjd', 'monthjob', 'mot', 'mov', 'mq', 'ms', 'mssql', 'msword', 'much', 'mult', 'multicult', 'multin', 'multipl', 'multitask', 'must', 'mvc', 'mysql', 'nam', 'nat', 'navig', 'near', 'necess', 'nee', 'negoty', 'net', 'network', 'nev', 'next', 'nic', 'nimbl', 'nin', 'nod', 'nondiscrimin', 'nonfunct', 'nontechn', 'norm', 'nosql', 'num', 'numb', 'object', 'obtain', 'occas', 'offsh', 'onboard', 'oncal', 'one hundred', 'ongo', 'onlin', 'onprem', 'onsit', 'ontim', 'op', 'opportun', 'opt', 'optim', 'orac', 'orchest', 'ord', 'org', 'origin', 'ory', 'os', 'oth', 'outcom', 'outlook', 'output', 'outsid', 'outsourc', 'outstand', 'overal', 'overs', 'oversea', 'overview', 'pac', 'pack', 'pag', 'pap', 'par', 'part', 'particul', 'partn', 'party', 'pass', 'past', 'patch', 'path', 'pattern', 'paty', 'pay', 'pc', 'pci', 'pcs', 'peer', 'penet', 'per', 'perfect', 'perform', 'period', 'periph', 'perl', 'perm', 'permanents', 'permit', 'person', 'personnel', 'perspect', 'persuas', 'pertain', 'pet', 'phas', 'phon', 'photo', 'photograph', 'photoshop', 'php', 'phys', 'pick', 'pipelin', 'plac', 'plan', 'platform', 'play', 'plsql', 'plu', 'pm', 'pmp', 'poc', 'point', 'policy', 'port', 'portfolio', 'posit', 'poss', 'possess', 'post', 'pot', 'pow', 'powerpoint', 'powershel', 'pr', 'pract', 'predict', 'pref', 'prefer', 'prem', 'prep', 'pres', 'press', 'prev', 'prevy', 'pri', 'pric', 'prid', 'prim', 'princip', 'principl', 'print', 'priorit', 'priv', 'proact', 'problem', 'problemsolv', 'proc', 'process', 'produc', 'profess', 'proficy', 'profil', 'profit', 'program', 'progress', 'project', 'promot', 'prompt', 'proof', 'prop', 'property', 'propos', 'proposit', 'prospect', 'protect', 'protocol', 'prototyp', 'proud', 'prov', 'provid', 'proxy', 'publ', 'purchas', 'purpos', 'pursu', 'push', 'put', 'python', 'qa', 'qual', 'quart', 'query', 'quest', 'quick', 'quinnox', 'quot', 'r', 'rac', 'rais', 'rapid', 'rat', 'rdbms', 'reach', 'react', 'read', 'ready', 'real', 'realtim', 'reason', 'rec', 'receiv', 'recogn', 'recommend', 'recont', 'record', 'recovery', 'recruit', 'red', 'reduc', 'ref', 'refin', 'refresh', 'reg', 'regard', 'regardless', 'regress', 'regret', 'regul', 'releas', 'relig', 'rely', 'remain', 'remedy', 'remot', 'remov', 'remun', 'renew', 'repair', 'replac', 'reply', 'report', 'reposit', 'repres', 'reput', 'request', 'requir', 'requisit', 'research', 'resolv', 'resourc', 'respect', 'respond', 'respons', 'rest', 'result', 'resy', 'retail', 'retain', 'return', 'reus', 'revenu', 'review', 'revolv', 'reward', 'right', 'ring', 'risk', 'rm', 'roadmap', 'robot', 'robust', 'rol', 'rollout', 'room', 'root', 'rot', 'rout', 'routin', 'ruby', 'rul', 'run', 'saa', 'saf', 'salesforc', 'san', 'sap', 'satisfact', 'satisfy', 'sav', 'savvy', 'say', 'scal', 'scan', 'scenario', 'schedule', 'scheme', 'school', 'sci', 'scienceinform', 'scop', 'screen', 'script', 'scrum', 'sdlc', 'sdn', 'sea', 'seamless', 'search', 'searchelectcom', 'searchelectcomsearchelect', 'sec', 'second', 'sect', 'see', 'seek', 'seg', 'sel', 'select', 'selen', 'self', 'selfdr', 'selfmot', 'selfstart', 'send', 'sens', 'sensit', 'seny', 'serv', 'sery', 'sess', 'set', 'setup', 'sev', 'sex', 'shal', 'shap', 'shar', 'sharepoint', 'shift', 'ship', 'shoot', 'short', 'shortl', 'show', 'sid', 'siem', 'sign', 'sim', 'simil', 'simpl', 'simult', 'singap', 'singaporeabout', 'singaporeemploy', 'singl', 'sint', 'sit', 'situ', 'six', 'six thousand', 'six thousand, five hundred and eighty-nine', 'sixty', 'sixty-five', 'siz', 'skil', 'skills', 'skillset', 'skillsgood', 'sla', 'smal', 'smart', 'sme', 'smoo', 'soap', 'soc', 'socy', 'soft', 'softw', 'sol', 'solid', 'solv', 'someon', 'someth', 'soon', 'sou', 'sound', 'sourc', 'spac', 'span', 'spark', 'speak', 'spec', 'spee', 'spend', 'spirit', 'spok', 'spons', 'spring', 'sprint', 'sql', 'stabl', 'stack', 'staff', 'stag', 'stakehold', 'stand', 'start', 'startup', 'stat', 'stay', 'step', 'stil', 'stor', 'story', 'strategic', 'strategies', 'strategy', 'stream', 'streamline', 'strive', 'strong', 'strongly', 'structure', 'strut', 'stud', 'studio', 'study', 'styl', 'subject', 'submit', 'success', 'suggest', 'suit', 'sum', 'superv', 'supervid', 'supery', 'supply', 'support', 'sur', 'survey', 'sustain', 'svn', 'swift', 'switch', 'system', 'tabl', 'tableau', 'tackl', 'tact', 'tail', 'tak', 'tal', 'talk', 'target', 'task', 'tax', 'tcpip', 'tdd', 'teach', 'team', 'teamwork', 'tech', 'techn', 'technolog', 'tel', 'telecom', 'telecommun', 'telephon', 'temp', 'templ', 'ten', 'tend', 'term', 'territ', 'test', 'thank', 'thing', 'think', 'third', 'thirdparty', 'thirteen', 'thirty', 'thirty-five', 'thorough', 'threat', 'threats', 'three', 'three hundred and sixty-five', 'three thousand', 'three thousand, five hundred', 'thrive', 'throughout', 'ticket', 'tier', 'tight', 'tim', 'timefram', 'timelin', 'titl', 'today', 'togeth', 'tomc', 'tool', 'top', 'tot', 'toward', 'track', 'trad', 'traff', 'train', 'transact', 'transf', 'transform', 'transit', 'transl', 'transp', 'transport', 'travel', 'tre', 'trend', 'tri', 'tripartit', 'troubl', 'troubleshoot', 'tru', 'trust', 'tun', 'turn', 'twelv', 'twenty', 'twenty-five', 'twenty-four', 'twenty-three', 'two', 'two hundred and forty-seven', 'two thousand', 'two thousand and eight', 'two thousand and nineteen', 'two thousand and sixteen', 'two thousand and twelve', 'two thousand, five hundred', 'typ', 'uat', 'ui', 'uiux', 'un', 'understand', 'undertak', 'unit', 'univers', 'unix', 'upd', 'upgrad', 'upon', 'uptod', 'urg', 'util', 'ux', 'valid', 'valu', 'vary', 'vbnet', 'vend', 'ver', 'verb', 'vers', 'vert', 'vet', 'viabl', 'video', 'view', 'virt', 'vis', 'visit', 'vmware', 'voic', 'volum', 'vpn', 'vuln', 'wan', 'want', 'wareh', 'way', 'web', 'webbas', 'weblog', 'webserv', 'websit', 'websph', 'week', 'weekend', 'wel', 'welcom', 'wheth', 'whilst', 'whol', 'wid', 'wil', 'win', 'window', 'wirefram', 'wireless', 'wish', 'within', 'without', 'word', 'work', 'workflow', 'workforc', 'workload', 'workplac', 'workshop', 'workst', 'world', 'worldclass', 'worldwid', 'would', 'writ', 'xml', 'yearsloc', 'yr']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix.shape)\n",
    "print(tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"20\">4.0 Finding Number of Clusters Elbow method</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-175bc3972b07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mwcss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    969\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy_x\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m                 return_n_iter=True)\n\u001b[0m\u001b[0;32m    972\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36mk_means\u001b[1;34m(X, n_clusters, sample_weight, init, precompute_distances, n_init, max_iter, verbose, tol, random_state, copy_x, n_jobs, algorithm, return_n_iter)\u001b[0m\n\u001b[0;32m    378\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecompute_distances\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_squared_norms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_squared_norms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                 random_state=random_state)\n\u001b[0m\u001b[0;32m    381\u001b[0m             \u001b[1;31m# determine if these results are the best so far\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py\u001b[0m in \u001b[0;36m_kmeans_single_lloyd\u001b[1;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             centers = _k_means._centers_sparse(X, sample_weight, labels,\n\u001b[1;32m--> 551\u001b[1;33m                                                n_clusters, distances)\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m             centers = _k_means._centers_dense(X, sample_weight, labels,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = [] \n",
    "pbar = tqdm.tqdm(total=15)\n",
    "\n",
    "for i in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "    kmeans.fit(tfidf_matrix)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    pbar.update(1)\n",
    "pbar.close()\n",
    "plt.plot(range(1,15),wcss)\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.savefig('elbow(k=1,15).png')\n",
    "plt.show()\n",
    "\n",
    "#plot the graph with \n",
    "#Sum of squared distances of samples to their closest cluster center.#Within-Cluster-Sum-of-Squares\n",
    "#agaisnt\n",
    "#num of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : <br/>\n",
    "I decide to use K = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"20\">5.0 K-Means Clustering</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to backup kmeans model variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('km_model(k=8)', 'wb') as bf:\n",
    "    pickle.dump(km, bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to load data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('km_model(k=8)', 'rb') as lf:\n",
    "    km = pickle.load(lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 8\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_adverts = { 'title': job_adverts_title, 'desc': job_adverts_desc, 'cluster': clusters }\n",
    "\n",
    "frame = pd.DataFrame(job_adverts, index = [clusters] , columns = ['title', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    947\n",
       "1    771\n",
       "3    767\n",
       "0    719\n",
       "6    708\n",
       "4    696\n",
       "2    129\n",
       "5     90\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame['cluster'].value_counts() #number of job_adverts per cluster (clusters from 0 to 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f2b895d358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "CountStatus = frame['cluster'].value_counts()\n",
    "CountStatus.plot.barh()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uSER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " networking, support, hardware, systems, service, troubleshooting, provide, security, administration, configuration, computing, installation, management, windows, operational, software, technical, issues, requirements, infrastructure, work, performance, resolve, monitoring, maintain, maintenance, knowledge, desktop, problem, backup,\n",
      "\n",
      "Cluster 0 titles:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uSER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IT Desktop Support | Local MNC | Central, Cloud Architect | Local MNC | Central, Network Architect | Local MNC | Central, IT Customer Service Officer (UBI/UP to $2300/5 days, VMware Engineer, System Engineer (VCAP/ VMware/ up to $5.5K), Presales Network Engineer x 2 (West/ SI/ CCNA/ CCNP/Cisco/ Up to 4.5K), Technical Engineer ( Desktop cum Network support ), IT SUPPORT/HELPDESK OFFICER $2800+BONUS, TROUBLESHOOT DESKTOP/OS @SIMEI/EAST, System / Server Administrator,\n",
      "\n",
      "Cluster 1 words: management, project, business, teams, processes, ensure, work, solve, requirements, developed, leading, plans, delivery, skills, service, customer, support, communication, responsible, active, operational, stakeholders, analytical, technical, implementing, provide, risk, technology, strong, quality,\n",
      "\n",
      "Cluster 1 titles: Game Producer | Up to $7000 basic | MNC, SAP Manager and Senior Consultants, Senior Game Producer | Up to $8000 basic | MNC, Head, Infrastructure, Senior Software Developer, Enterprise Account Manager, Sales Manager, Junior Project Manager (1 year contract), Assistant / Security Manager (Planning & Projects), Technical Project Manager / Technical Lead, Project Manager (Internal Ref. 17000149),\n",
      "\n",
      "Cluster 2 words: infotech, greet, infotechposition, regarding, monthjd, yearslocation, client, information, operational, singaporeemployer, renewal, selection, contract, replication, candidates, two, word, four, format, net, eight, updated, three, profiling, jd, centre, systems, fifteen, java, service,\n",
      "\n",
      "Cluster 2 titles: C++ Developer, Data Center Operations, Full Stack Web Developer, Technology Ops Support Practioner, Dotnet Developer, Customer Service, IT Helpdesk Engineer, Dotnet Developer, Senior .Net Developer, Customer Service,\n",
      "\n",
      "Cluster 3 words: teams, work, productionize, business, growth, markets, developed, customer, management, build, opportunity, leading, solve, technology, help, world, software, skills, making, engineer, learning, data, communication, service, role, looking, digital, global, industry, responsible,\n",
      "\n",
      "Cluster 3 titles: Data Scientist (Operation) | Local MNC | Central, DevOps Engineer (up to RM14k), Customer Account Manager/ Account executive, AM IT Singapore Head (IT Project Manager) , Asset Management IT, Signalling Site Engineer, Product Designer, Associate Consultant, Associate Product Manager, Systems Analyst / Software Engineer, Systems Analyst / Software Engineer,\n",
      "\n",
      "Cluster 4 words: testing, developed, requirements, systems, design, analytical, software, project, documentations, work, solve, business, specifications, teams, technical, function, programming, implementing, management, productionize, support, skills, knowledge, quality, integration, ensure, processes, automating, responsible, code,\n",
      "\n",
      "Cluster 4 titles: Data Scientist (Optimization) | Local MNC | Central, Application Engineer (IT Solution @ PJU1A - Up to RM6000), Software Engineer / Programmer| Up to $7k, (SA) Software Engineer - JAVA [Ref: 43786] Cyberjaya, RM6k, (SA) Software Engineer - C# [Ref: 43785] - Putrajaya/Cyberjaya, [SA] Java Software / Application Engineer [Ref 44191] RM 6k - 10k, Performance Test Engineer (Up to $7500/ Python Language/ Linux Environment/ SQL), Software Engineer ( C#/ C++), System Analyst (Based in Kuala Lumpur), Software & QA Support Engineer (Based in Kuala Lumpur),\n",
      "\n",
      "Cluster 5 words: security, searchelectcom, cyber, tripartite, daryl, nondiscriminatory, manpower, consulting, discriminate, thank, feel, cooperative, series, raising, alternate, singapore, immediate, client, searchelectcomsearchelect, information, min, threat,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uSER\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " six, guidelines, penetration, adhere, following, assess, matter, contacted,\n",
      "\n",
      "Cluster 5 titles: Communications Solution Architect (Presales), Regional Marketing Manager, APAC, Sales Engineering Manager, Senior Sales Manager, Pega Developer, SOC Manager, Cybersecurity Specialist, Associate Security Consultant, Security Consultant, Security Lead,\n",
      "\n",
      "Cluster 6 words: developed, web, design, apicentric, java, javascript, code, frameworks, testing, work, knowledge, technology, spring, software, rest, service, mobile, programming, skills, requirements, net, understanding, databases, strong, css, architecture, systems, good, teams, sql,\n",
      "\n",
      "Cluster 6 titles: Web / Front End Developer (up to S$7000, Proficient in VUE, Angular, React), PHP Developer / Software Engineer (KL), Web Application Developer (.NET) Central, Kuala Lumpur, Senior .Net Developer, Lead Software Engineer, IT Manager - Java Applications, Software Engineer - PE-JD-640, Artificial Intelligence Bot Engineer Intern, Software Engineer - Web (JavaScript / React), Singapore, Sr. Software Engineer,\n",
      "\n",
      "Cluster 7 words: work, skills, requirements, management, candidates, developed, good, communication, knowledge, data, project, software, rm, support, business, analytical, sap, least, two, preferred, must, design, teams, client, customer, information, types, service, three, strong,\n",
      "\n",
      "Cluster 7 titles: Business Process Engineer | Local MNC | Central, Data Scientist (M&R;) | Local MNC | Central, Video Algorithm Engineer (AI/Streaming/C++/High Pay/Multiple openings), [Govt Sector] IT Software Trainer - up to $2500/ Mon to Fri, Business / Systems Analyst (Logistics / UP $8000 / West / 5 Days / 5-8 Yrs Exp), Business / System Analyst Leader (Up $8000 / 5 Days / 4-8 Yrs Exp / AWS+VB), Performance Test Engineer (Up to $7500 / Python / Linux / US MNC), SAP BPC Functional Consultant, [SA] Project Assistant Manager for service industry (ref:44850), 7k - 8k, BROADCAST MOTION GRAPHIC DESIGNER -VIDEO EDITING&3D $2500+ALLOWANCE+AWS,\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "terms = tf.get_feature_names()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :30]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0], end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist()[:10]:\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "r = lambda: random.randint(0,255)\n",
    "\n",
    "cluster_names = [\n",
    "'networking, support, hardware, systems, service, troubleshooting, security, configuration, computing, installation, windows, operational, technical, issues, monitoring',\n",
    "\n",
    "'management, project, business, ensure, solve, requirements, plans, delivery, skills, customer, communication, stakeholders, analytical, quality, strong',\n",
    "\n",
    "'infotech, operational, singaporeemployer, renewal, selection, replication, candidates',\n",
    "\n",
    "'teams, work, productionize, business, growth, markets, customer, management, learning, data, software, skills, communication, solve',\n",
    "\n",
    "'testing, requirements, system, design, analytical. software, project, documentations, business, specifications, technical, function, programming, automating, quality, integration',\n",
    "\n",
    "'security, cyber, tripartite, daryl, nondiscriminatory, consulting, cooperative, threat, min, discriminate',\n",
    "\n",
    "'developed, web, design, apicentric, java, javascript, code, frameworks, testing, work, net, databases, strong, css, good, sql, teams',\n",
    "\n",
    "'management, candidates, developed, requirements, management, communication, knowledge, data, project, software, support, business, analytical, sap',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cluster_colors = []\n",
    "num_of_clusters = 8\n",
    "\n",
    "for num in range(num_of_clusters):\n",
    "    cluster_colors.append('#%02X%02X%02X' % (r(),r(),r()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os  # for os.path.basename\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "MDS()\n",
    "\n",
    "# convert two components as we're plotting points in a two-dimensional plane\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# we will also specify `random_state` so the plot is reproducible.\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1, verbose=True)\n",
    "\n",
    "pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to backup kmeans model variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ys', 'wb') as bf:\n",
    "    pickle.dump(ys, bf)\n",
    "with open('xs', 'wb') as bf:\n",
    "    pickle.dump(xs, bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to load data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ys', 'rb') as lf:\n",
    "    ys = pickle.load(lf)\n",
    "with open('xs', 'rb') as lf:\n",
    "    xs = pickle.load(lf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mpld3\n",
    "\n",
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_plots = 2000\n",
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs[:num_of_plots], y=ys[:num_of_plots], \n",
    "                       label=clusters[:num_of_plots], \n",
    "                       title=job_adverts_title[:num_of_plots]))\n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "\n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\n",
    "svg.mpld3-figure {\n",
    "margin-left: 200px;}\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(20,12)) #set plot size\n",
    "ax.margins(0.03) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "                     label=cluster_names[name], mec='none', \n",
    "                     color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "mpld3.display() #show the plot\n",
    "\n",
    "#uncomment the below to export to html\n",
    "html = mpld3.fig_to_html(fig)\n",
    "#import pickle\n",
    "with open('mpld3_k8.html', 'wb') as bf:\n",
    "    pickle.dump(html, bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"25\">View the exported D3.html !</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
