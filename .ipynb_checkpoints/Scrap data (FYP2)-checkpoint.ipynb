{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import Request, urlopen\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">Set Up MongoDB for data storage</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "try: \n",
    "    client = MongoClient()\n",
    "    print(\"Connected successfully!!!\") \n",
    "except:\n",
    "    print(\"Could not connect to MongoDB\") \n",
    "\n",
    "db = client.job_advert_db #select database\n",
    "collection = db.job_advert #select collection to insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">-------------</font>\n",
    "<font size=\"30\">JobStreet Scraping</font>\n",
    "<font size=\"30\">-------------</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#timestamping\n",
    "timestamp = datetime.datetime.now().date()\n",
    "###textFile = open('D:/jobstreet_data_' + timestamp + '.txt', 'a', encoding=\"utf-8\")\n",
    "###textFile.write('Data scraped at >> ' + str(timestamp) + 'on Jobstreet')\n",
    "#we have to specify the headers to prevent https security 403 error occurs\n",
    "total_paging = 955\n",
    "advert_per_page = 20\n",
    "pbar = tqdm.tqdm(total=int(total_paging/advert_per_page))\n",
    "paging = int(total_paging/advert_per_page)\n",
    "decrement = 1\n",
    "\n",
    "#scraped area is whole malaysia and singapore (all location) because jobstreet doesn't cover much other areas\n",
    "while paging > -1 :\n",
    "    req = Request('https://www.jobstreet.com.my/en/job-search/job-vacancy.php?area=1&option=1&location=&specialization=191%2C192%2C193&job-source=3%2C64&classified=1&job-posted=0&sort=1&order=0&src=16&srcr=90&ojs=4&pg='+ str(paging), headers={'User-Agent': 'Chrome/60.0.3112.90'})\n",
    "    webpage = urlopen(req)\n",
    "    html_webpage = webpage.read()\n",
    "    soup_page = bs(html_webpage, 'html.parser')\n",
    "    #scrap all the texts from the job description div element, <div class=\"unselectable wrap-text\" id=\"job_description\">\n",
    "    for div in soup_page.findAll('a', {'class':'position-title-link'}):\n",
    "        #remove the redundant link\n",
    "        #open the job details link\n",
    "        if div.get('href') != 'https://www.jobstreet.com.my/en/job/1':\n",
    "            link = div.get('href')\n",
    "            accessible_link = link.split(\"?\")[0] #seperate out the link param that will make the site being forbid\n",
    "            req = Request(accessible_link, headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "            job_desc_page = urlopen(req)\n",
    "            html_job_desc_page = job_desc_page.read()\n",
    "            soup_job_desc_page = bs(html_job_desc_page, 'html.parser')\n",
    "            \n",
    "            #scraping job title and job description\n",
    "            job_title_h1 = soup_job_desc_page.find('h1', {'id':'position_title'})\n",
    "            job_desc_div = soup_job_desc_page.find('div', {'id':'job_description'})\n",
    "            \n",
    "            ###job_salary_span = soup_job_desc_page.find('span', {'id': 'salary_range'})\n",
    "            #the html page contains none h1 element\n",
    "            ###textFile = open('D:/jobstreet_data_' + str(timestamp) + '.txt', 'a', encoding=\"utf-8\")\n",
    "            ###if job_desc_div != None:\n",
    "            ###    text_data = str(job_desc_div.text)\n",
    "            ###    textFile.write(text_data + '\\n**data**\\n')\n",
    "            ###textFile.close()\n",
    "            if job_desc_div != None:\n",
    "                job_advert_data = { \n",
    "                    \"title\": str(job_title_h1.text),\n",
    "                    \"desc\": str(job_desc_div.text),\n",
    "                    #\"salary\": str(job_salary_.text),\n",
    "                    \"date_rec\": str(timestamp),\n",
    "                    \"source\": \"jobstreet\"\n",
    "                }\n",
    "                rec_data = collection.insert_one(job_advert_data)\n",
    "                \n",
    "        if paging > -1 :\n",
    "            paging = paging - decrement\n",
    "            pbar.update(decrement)\n",
    "            print(paging)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">---------------</font>\n",
    "<font size=\"30\">Indeed Scraping</font>\n",
    "<font size=\"30\">---------------</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#timestamping\n",
    "timestamp = datetime.datetime.now().date()\n",
    "###textFile = open('D:/indeed_data_' + timestamp + '.txt', 'a', encoding=\"utf-8\")\n",
    "###textFile.write('\\nData scraped at >> ' + str(timestamp) + ' on Indeed\\n')\n",
    "###textFile.close()\n",
    "\n",
    "pbar = tqdm.tqdm(total=910)\n",
    "paging = 910\n",
    "decrement = 10\n",
    "#href links from 1st page of jobstreet\n",
    "#scrap all the texts from the job description a element, <a class=\"unselectable wrap-text\" id=\"job_description\">\n",
    "while paging > -1 :\n",
    "    req = Request('https://www.indeed.com.sg/jobs?q=IT+Software&l=Singapore&start=' + str(int(paging)), headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "    #req = Request('https://www.indeed.com.my/jobs?q=IT+Software&l=Kuala+Lumpur&start=' + str(int(paging)), headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "    webpage = urlopen(req)\n",
    "    html_webpage = webpage.read()\n",
    "    soup_page = bs(html_webpage, 'html.parser')\n",
    "    for h2 in soup_page.findAll('h2', {'class':'jobtitle'}):\n",
    "        #remove the redundant link & open the job details link\n",
    "        link = h2.a.get('href')\n",
    "        link = 'https://www.indeed.com.my' + link\n",
    "        req = Request(link, headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "        job_desc_page = urlopen(req)\n",
    "        html_job_desc_page = job_desc_page.read()\n",
    "        soup_job_desc_page = bs(html_job_desc_page, 'html.parser')\n",
    "        #scraping job title and job description\n",
    "        job_title_h3 = soup_job_desc_page.find('h3', {'class':'jobsearch-JobInfoHeader-title'})\n",
    "        job_desc_div = soup_job_desc_page.find('div', {'class':'jobsearch-JobComponent-description'})\n",
    "        job_salary_div = soup_job_desc_page.find('div', {'class':'jobsearch-JobMetadataHeader-item'})\n",
    "        \n",
    "        #the html page contains none h1 element\n",
    "        ###textFile = open('D:/indeed_data_' + str(timestamp) + '.txt', 'a', encoding=\"utf-8\")\n",
    "        ###if job_desc_div != None:\n",
    "        ###    text_data = str(job_desc_div.text)\n",
    "        ###   textFile.write(text_data)\n",
    "        ###textFile.close()\n",
    "        if job_desc_div != None:\n",
    "            job_advert_data = { \n",
    "                \"title\": str(job_title_h3.text),\n",
    "                \"desc\": str(job_desc_div.text),\n",
    "                \"salary\": str(job_salary_div.text) if job_salary_div != None else \"\",\n",
    "                \"date_rec\": str(timestamp),\n",
    "                \"source\": \"indeed\"\n",
    "            }\n",
    "            rec_data = collection.insert_one(job_advert_data) \n",
    "            \n",
    "    if paging > -1 :\n",
    "        paging = paging - decrement\n",
    "        pbar.update(decrement)\n",
    "        print(paging)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"30\">---------------</font>\n",
    "<font size=\"30\">Monster Scraping</font>\n",
    "<font size=\"30\">---------------</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestamping\n",
    "timestamp = datetime.datetime.now().date()\n",
    "###textFile = open('D:/monster_data_' + str(timestamp) + '.txt', 'a', encoding=\"utf-8\")\n",
    "###textFile.write('\\nData scraped at >> ' + str(timestamp) + ' on Monster\\n')\n",
    "###textFile.close()\n",
    "\n",
    "import tqdm\n",
    "total_paging = 757\n",
    "advert_per_page = 40\n",
    "pbar = tqdm.tqdm(total=int(total_paging/advert_per_page))\n",
    "paging = int(total_paging/advert_per_page)\n",
    "decrement = 1\n",
    "#href links from 1st page of jobstreet\n",
    "#scrap all the texts from the job description a element, <a class=\"unselectable wrap-text\" id=\"job_description\">\n",
    "while paging > 0 :\n",
    "    req = Request('https://www.monster.com.my/it-jobs-in-singapore-'+ str(paging) +'.html', headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "    #req = Request('https://www.monster.com.my/it-jobs-in-kuala-lumpur-'+ str(paging) +'.html', headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "    webpage = urlopen(req)\n",
    "    html_webpage = webpage.read()\n",
    "    soup_page = bs(html_webpage, 'html.parser')\n",
    "    for h2 in soup_page.findAll('h2', {'class':'seotitle'}):\n",
    "        #remove the redundant link & open the job details link\n",
    "        link = h2.a.get('href')\n",
    "        link = 'https:' + str(link)\n",
    "        req = Request(link, headers={'User-Agent': 'Chrome/59.0.3071.115'})\n",
    "        job_desc_page = urlopen(req)\n",
    "        html_job_desc_page = job_desc_page.read()\n",
    "        soup_job_desc_page = bs(html_job_desc_page, 'html.parser')\n",
    "        \n",
    "        #scraping job title and job description\n",
    "        job_title_h1 = soup_job_desc_page.find('h1', {'class':'job_title_seo'})\n",
    "        job_desc_div = soup_job_desc_page.find('div', {'class':'desc'})\n",
    "        job_salary_h1 = job_title_h1 #salary located with title together\n",
    "\n",
    "        ###textFile = open('D:/monster_data_' + str(timestamp) + '.txt', 'a', encoding=\"utf-8\")\n",
    "        ###if job_desc_div != None:\n",
    "        ###    text_data = str(job_desc_div.text)\n",
    "        ###    textFile.write(text_data)\n",
    "        ###textFile.close()\n",
    "        if job_title_h1 != None:\n",
    "            job_advert_data = { \n",
    "                \"title\": str(job_title_h1.text),\n",
    "                \"desc\": str(job_desc_div.text),\n",
    "                \"salary\": str(job_salary_h1.text),\n",
    "                \"date_rec\": str(timestamp),\n",
    "                \"source\": \"monster\"\n",
    "            }\n",
    "            rec_data = collection.insert_one(job_advert_data) \n",
    "            \n",
    "    if paging > 0 :\n",
    "        paging = paging - decrement\n",
    "        pbar.update(decrement)\n",
    "        print(paging)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
